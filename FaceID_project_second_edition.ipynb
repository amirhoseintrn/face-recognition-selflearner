{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e1ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5b519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering dataset for people with 3 or more images...\n",
      "Adding person to dataset: Ariel_Sharon with 76 images.\n",
      "Adding person to dataset: Arnold_Schwarzenegger with 41 images.\n",
      "Adding person to dataset: Colin_Powell with 235 images.\n",
      "Adding person to dataset: Donald_Rumsfeld with 121 images.\n",
      "Adding person to dataset: George_W_Bush with 529 images.\n",
      "Adding person to dataset: Gerhard_Schroeder with 108 images.\n",
      "Adding person to dataset: Gloria_Macapagal_Arroyo with 44 images.\n",
      "Adding person to dataset: Hugo_Chavez with 71 images.\n",
      "Adding person to dataset: Jacques_Chirac with 52 images.\n",
      "Adding person to dataset: Jean_Chretien with 55 images.\n",
      "Adding person to dataset: Jennifer_Capriati with 41 images.\n",
      "Adding person to dataset: John_Ashcroft with 53 images.\n",
      "Adding person to dataset: Junichiro_Koizumi with 60 images.\n",
      "Adding person to dataset: Laura_Bush with 41 images.\n",
      "Adding person to dataset: Lleyton_Hewitt with 40 images.\n",
      "Adding person to dataset: Luiz_Inacio_Lula_da_Silva with 48 images.\n",
      "Adding person to dataset: Serena_Williams with 50 images.\n",
      "Adding person to dataset: Tony_Blair with 144 images.\n",
      "Adding person to dataset: Vladimir_Putin with 48 images.\n",
      "\n",
      "Processing complete!\n",
      "Total number of images collected: 1857\n",
      "Number of unique people in the dataset: 19\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Set the path to the directory with your cropped faces\n",
    "cropped_data_path = 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\lfw_cropped_faces'\n",
    "\n",
    "# A list to hold the final processed image arrays\n",
    "images_with_three_or_more = []\n",
    "person_labels = []\n",
    "\n",
    "# Loop through each person's directory to count their images\n",
    "print(\"Filtering dataset for people with 3 or more images...\")\n",
    "for person_name in os.listdir(cropped_data_path):\n",
    "    person_dir = os.path.join(cropped_data_path, person_name)\n",
    "    if os.path.isdir(person_dir):\n",
    "        # Count the number of images for this person\n",
    "        image_files = [f for f in os.listdir(person_dir) if f.endswith(('.jpg', '.png'))]\n",
    "        \n",
    "        # If the person has 3 or more images, add them to our list\n",
    "        \n",
    "        if len(image_files) >= 40 :\n",
    "            print(f\"Adding person to dataset: {person_name} with {len(image_files)} images.\")\n",
    "            for filename in image_files:\n",
    "                img_path = os.path.join(person_dir, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                \n",
    "                if img is not None:\n",
    "                    # Convert to RGB and normalize\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = img / 255.0\n",
    "                    images_with_three_or_more.append(img)\n",
    "                    person_labels.append(person_name)\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n",
    "print(f\"Total number of images collected: {len(images_with_three_or_more)}\")\n",
    "print(f\"Number of unique people in the dataset: {len(set(person_labels))}\")\n",
    "\n",
    "# The list 'images_with_three_or_more' now contains your preprocessed image arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf382b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting lists to NumPy arrays...\n",
      "Conversion complete!\n",
      "Shape of image data: (1857, 128, 128, 3)\n",
      "Shape of labels: (1857,)\n",
      "\n",
      "Preparing labels...\n",
      "\n",
      "Building the balanced CNN model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57600</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,372,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,451</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57600\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m7,372,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)             │         \u001b[38;5;34m2,451\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,394,771</span> (28.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,394,771\u001b[0m (28.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,394,771</span> (28.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,394,771\u001b[0m (28.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Epoch 1/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 360ms/step - accuracy: 0.1767 - loss: 3.3588 - val_accuracy: 0.2715 - val_loss: 2.8162\n",
      "Epoch 2/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 334ms/step - accuracy: 0.2677 - loss: 2.6727 - val_accuracy: 0.2715 - val_loss: 2.6226\n",
      "Epoch 3/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 331ms/step - accuracy: 0.2791 - loss: 2.6043 - val_accuracy: 0.2715 - val_loss: 2.4873\n",
      "Epoch 4/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 338ms/step - accuracy: 0.2726 - loss: 2.6506 - val_accuracy: 0.2742 - val_loss: 2.4544\n",
      "Epoch 5/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 339ms/step - accuracy: 0.3098 - loss: 2.4267 - val_accuracy: 0.3360 - val_loss: 2.2353\n",
      "Epoch 6/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 319ms/step - accuracy: 0.3187 - loss: 2.2837 - val_accuracy: 0.3522 - val_loss: 2.0605\n",
      "Epoch 7/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 343ms/step - accuracy: 0.3647 - loss: 2.1088 - val_accuracy: 0.3871 - val_loss: 1.9120\n",
      "Epoch 8/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 330ms/step - accuracy: 0.4132 - loss: 1.8853 - val_accuracy: 0.3952 - val_loss: 1.8241\n",
      "Epoch 9/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 399ms/step - accuracy: 0.4216 - loss: 1.8050 - val_accuracy: 0.4731 - val_loss: 1.6599\n",
      "Epoch 10/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 329ms/step - accuracy: 0.4796 - loss: 1.6280 - val_accuracy: 0.5430 - val_loss: 1.4370\n",
      "Epoch 11/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 352ms/step - accuracy: 0.5029 - loss: 1.5214 - val_accuracy: 0.5887 - val_loss: 1.3787\n",
      "Epoch 12/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 352ms/step - accuracy: 0.5590 - loss: 1.3927 - val_accuracy: 0.6290 - val_loss: 1.3204\n",
      "Epoch 13/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 381ms/step - accuracy: 0.5604 - loss: 1.3440 - val_accuracy: 0.6452 - val_loss: 1.2244\n",
      "Epoch 14/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 354ms/step - accuracy: 0.5666 - loss: 1.2385 - val_accuracy: 0.7177 - val_loss: 1.0378\n",
      "Epoch 15/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 392ms/step - accuracy: 0.6100 - loss: 1.1296 - val_accuracy: 0.7070 - val_loss: 1.0298\n",
      "Epoch 16/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 380ms/step - accuracy: 0.6522 - loss: 1.0364 - val_accuracy: 0.7554 - val_loss: 0.9390\n",
      "Epoch 17/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 351ms/step - accuracy: 0.6642 - loss: 1.0196 - val_accuracy: 0.7204 - val_loss: 0.9835\n",
      "Epoch 18/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 379ms/step - accuracy: 0.6440 - loss: 0.9966 - val_accuracy: 0.7231 - val_loss: 0.9420\n",
      "Epoch 19/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 323ms/step - accuracy: 0.6807 - loss: 0.9148 - val_accuracy: 0.7392 - val_loss: 0.9398\n",
      "Epoch 20/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 346ms/step - accuracy: 0.6575 - loss: 0.9532 - val_accuracy: 0.7527 - val_loss: 0.9803\n",
      "Epoch 21/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 342ms/step - accuracy: 0.7013 - loss: 0.8455 - val_accuracy: 0.7688 - val_loss: 0.8932\n",
      "Epoch 22/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 323ms/step - accuracy: 0.7184 - loss: 0.7972 - val_accuracy: 0.7608 - val_loss: 0.9335\n",
      "Epoch 23/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 306ms/step - accuracy: 0.6970 - loss: 0.8630 - val_accuracy: 0.7661 - val_loss: 0.9393\n",
      "Epoch 24/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 303ms/step - accuracy: 0.7236 - loss: 0.7916 - val_accuracy: 0.7527 - val_loss: 0.9645\n",
      "Epoch 25/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 300ms/step - accuracy: 0.7178 - loss: 0.7988 - val_accuracy: 0.7608 - val_loss: 0.9794\n",
      "Epoch 26/40\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 300ms/step - accuracy: 0.7387 - loss: 0.7238 - val_accuracy: 0.7715 - val_loss: 0.9610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved successfully as 'my_lfw_filtered_model.h5'\n",
      "\n",
      "Final model weights saved successfully as 'my_face_id_model_final_weights.h5'\n",
      "\n",
      "Embedding model ready for 128-dimensional feature extraction\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "print(\"Converting lists to NumPy arrays...\")\n",
    "\n",
    "X_data = np.array(images_with_three_or_more)\n",
    "y_labels_names = np.array(person_labels)\n",
    "print(\"Conversion complete!\")\n",
    "print(f\"Shape of image data: {X_data.shape}\")\n",
    "print(f\"Shape of labels: {y_labels_names.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\nPreparing labels...\")\n",
    "le = LabelEncoder()\n",
    "y_labels_encoded = le.fit_transform(y_labels_names)\n",
    "y_labels_one_hot = to_categorical(y_labels_encoded)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_labels_one_hot, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nBuilding the balanced CNN model...\")\n",
    "num_classes = y_labels_one_hot.shape[1]\n",
    "\n",
    "\n",
    "inputs = Input(shape=X_train.shape[1:])\n",
    "\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "\n",
    "embedding_output = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(embedding_output)\n",
    "\n",
    "\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# ایجاد مدل\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,  # Add the target labels here\n",
    "    epochs=40, \n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "# 5. ذخیره مدل\n",
    "model.save('my_lfw_filtered_model.h5')\n",
    "print(\"\\nModel saved successfully as 'my_lfw_filtered_model.h5'\")\n",
    "model.save_weights('my_face_id_model_final.weights.h5')\n",
    "print(\"\\nFinal model weights saved successfully as 'my_face_id_model_final_weights.h5'\")\n",
    "\n",
    "# 6. ایجاد مدل برای استخراج امبدینگ\n",
    "embedding_model = Model(inputs=model.input, outputs=embedding_output)\n",
    "print(\"\\nEmbedding model ready for 128-dimensional feature extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.7752 - loss: 0.9150\n",
      "دقت مدل روی کل داده تست: 0.7688\n",
      "Loss روی کل داده تست: 0.8932\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step\n",
      "تعداد نمونه های تست: 372\n",
      "تعداد پیش‌بینی های صحیح: 286\n",
      "دقت محاسبه شده دستی: 0.7688\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"test accuracy{test_accuracy:.4f}\")\n",
    "print(f\"loss function value {test_loss:.4f}\")\n",
    "\n",
    "\n",
    "all_predictions = model.predict(X_test, verbose=1)\n",
    "predicted_classes = np.argmax(all_predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "correct_predictions = np.sum(predicted_classes == true_classes)\n",
    "total_samples = len(true_classes)\n",
    "manual_accuracy = correct_predictions / total_samples\n",
    "\n",
    "print(f\"number of samples for testin {total_samples}\")\n",
    "print(f\"number of correct predictions {correct_predictions}\")\n",
    "print(f\"the prececion {manual_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad166cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete!\n",
      "Shape of image data: (1857, 128, 128, 3)\n",
      "Shape of labels: (1857,)\n",
      "\n",
      "Preparing labels...\n",
      "Loading the saved model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Embedding model ready for 128-dimensional feature extraction\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "X_data = np.array(images_with_three_or_more)\n",
    "y_labels_names = np.array(person_labels)\n",
    "print(\"Conversion complete!\")\n",
    "print(f\"Shape of image data: {X_data.shape}\")\n",
    "print(f\"Shape of labels: {y_labels_names.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\nPreparing labels...\")\n",
    "le = LabelEncoder()\n",
    "y_labels_encoded = le.fit_transform(y_labels_names)\n",
    "y_labels_one_hot = to_categorical(y_labels_encoded)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_labels_one_hot, test_size=0.2, random_state=42\n",
    ")\n",
    "# Load the previously saved model\n",
    "print(\"Loading the saved model...\")\n",
    "loaded_model = tf.keras.models.load_model('my_lfw_filtered_model.h5')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "embedding_layer = loaded_model.get_layer('dense_8') # Assuming 'dense_1' is your embedding layer\n",
    "embedding_model = Model(inputs=loaded_model.input, outputs=embedding_layer.output)\n",
    "print(\"\\nEmbedding model ready for 128-dimensional feature extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18413ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Tested with 15 pairs of same-class images.\n",
      "Tested with 15 pairs of different-class images.\n",
      "\n",
      "Average distance for same-class images: 0.3111\n",
      "Average distance for different-class images: 0.6878\n",
      "\n",
      "Average difference in distances: 0.3767\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def cosine_distance(emb1, emb2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine distance between two embedding vectors.\n",
    "    A smaller distance indicates greater similarity.\n",
    "    \"\"\"\n",
    "    return 1 - np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "\n",
    "def find_indices_for_class(labels, target_class, num_samples=2):\n",
    "    \"\"\"\n",
    "    Finds the indices of images belonging to a specific class.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for i in range(len(labels)):\n",
    "        # np.argmax converts the one-hot encoded label back to a class index\n",
    "        if np.argmax(labels[i]) == target_class:\n",
    "            indices.append(i)\n",
    "            if len(indices) == num_samples:\n",
    "                break\n",
    "    return indices\n",
    "\n",
    "def compare_distances(embedding_model, X_test, y_test, num_classes_to_test=15):\n",
    "    \"\"\"\n",
    "    Compares the average cosine distance of same-class and different-class embeddings.\n",
    "    \"\"\"\n",
    "    same_class_distances = []\n",
    "    diff_class_distances = []\n",
    "    \n",
    "    unique_classes = np.unique(np.argmax(y_test, axis=1))\n",
    "\n",
    "    for i in range(num_classes_to_test):\n",
    "        if i + 1 >= len(unique_classes):\n",
    "            print(\"Warning: Not enough unique classes in y_test to run all tests.\")\n",
    "            break\n",
    "            \n",
    "        # Find two images from the same class\n",
    "        target_class = unique_classes[i]\n",
    "        same_indices = find_indices_for_class(y_test, target_class, num_samples=2)\n",
    "\n",
    "        # Skip if there are not enough images for this class\n",
    "        if len(same_indices) < 2:\n",
    "            print(f\"Skipping class {target_class}: Not enough samples.\")\n",
    "            continue\n",
    "        \n",
    "        # Extract embeddings and calculate distance\n",
    "        emb1 = embedding_model.predict(X_test[same_indices[0]:same_indices[0]+1])[0]\n",
    "        emb2 = embedding_model.predict(X_test[same_indices[1]:same_indices[1]+1])[0]\n",
    "        distance_same = cosine_distance(emb1, emb2)\n",
    "        same_class_distances.append(distance_same)\n",
    "\n",
    "        # Find two images from different classes\n",
    "        target_class_A = unique_classes[i]\n",
    "        target_class_B = unique_classes[i+1] # Using the next class for comparison\n",
    "        \n",
    "        diff_index_A = find_indices_for_class(y_test, target_class_A, num_samples=1)[0]\n",
    "        diff_index_B = find_indices_for_class(y_test, target_class_B, num_samples=1)[0]\n",
    "        \n",
    "        # Extract embeddings and calculate distance\n",
    "        emb3 = embedding_model.predict(X_test[diff_index_A:diff_index_A+1])[0]\n",
    "        emb4 = embedding_model.predict(X_test[diff_index_B:diff_index_B+1])[0]\n",
    "        distance_diff = cosine_distance(emb3, emb4)\n",
    "        diff_class_distances.append(distance_diff)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Tested with {len(same_class_distances)} pairs of same-class images.\")\n",
    "    print(f\"Tested with {len(diff_class_distances)} pairs of different-class images.\")\n",
    "\n",
    "    if same_class_distances:\n",
    "        average_same = np.mean(same_class_distances)\n",
    "        print(f\"\\nAverage distance for same-class images: {average_same:.4f}\")\n",
    "    \n",
    "    if diff_class_distances:\n",
    "        average_diff = np.mean(diff_class_distances)\n",
    "        print(f\"Average distance for different-class images: {average_diff:.4f}\")\n",
    "\n",
    "    if same_class_distances and diff_class_distances:\n",
    "        average_difference = average_diff - average_same\n",
    "        print(f\"\\nAverage difference in distances: {average_difference:.4f}\")\n",
    "compare_distances(embedding_model, X_test, y_test, num_classes_to_test=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d97ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7752 - loss: 0.9150\n",
      "accuracy on all of the test data 0.7688\n",
      "loss on all of the test data 0.8932\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step\n",
      "number of samples for testin 372\n",
      "number of correct predictions 286\n",
      "the prececion 0.7688\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, test_accuracy = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"accuracy on all of the test data {test_accuracy:.4f}\")\n",
    "print(f\"loss on all of the test data {test_loss:.4f}\")\n",
    "\n",
    "\n",
    "all_predictions = loaded_model.predict(X_test, verbose=1)\n",
    "predicted_classes = np.argmax(all_predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "correct_predictions = np.sum(predicted_classes == true_classes)\n",
    "total_samples = len(true_classes)\n",
    "manual_accuracy = correct_predictions / total_samples\n",
    "\n",
    "print(f\"number of samples for testin {total_samples}\")\n",
    "print(f\"number of correct predictions {correct_predictions}\")\n",
    "print(f\"the prececion {manual_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e80f9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory 'C:\\Exercises\\my pics' and output directory 'C:\\Exercises\\my pics\\cropped_resized_pics' are ready.\n",
      "Found 1 face(s) in 1.jpg. Processing...\n",
      "  - Saved cropped and resized face to C:\\Exercises\\my pics\\cropped_resized_pics\\1_face_0.jpg\n",
      "Found 1 face(s) in 2.jpg. Processing...\n",
      "  - Saved cropped and resized face to C:\\Exercises\\my pics\\cropped_resized_pics\\2_face_0.jpg\n",
      "Found 1 face(s) in 3.jpg. Processing...\n",
      "  - Saved cropped and resized face to C:\\Exercises\\my pics\\cropped_resized_pics\\3_face_0.jpg\n",
      "Found 1 face(s) in 4.jpg. Processing...\n",
      "  - Saved cropped and resized face to C:\\Exercises\\my pics\\cropped_resized_pics\\4_face_0.jpg\n",
      "Found 1 face(s) in 5.jpg. Processing...\n",
      "  - Saved cropped and resized face to C:\\Exercises\\my pics\\cropped_resized_pics\\5_face_0.jpg\n",
      "Found 1 face(s) in 6.jpg. Processing...\n",
      "  - Saved cropped and resized face to C:\\Exercises\\my pics\\cropped_resized_pics\\6_face_0.jpg\n",
      "Found 1 face(s) in 7.jpg. Processing...\n",
      "  - Saved cropped and resized face to C:\\Exercises\\my pics\\cropped_resized_pics\\7_face_0.jpg\n",
      "Found 1 face(s) in 8.jpg. Processing...\n",
      "  - Saved cropped and resized face to C:\\Exercises\\my pics\\cropped_resized_pics\\8_face_0.jpg\n",
      "Skipping non-image file: cropped_resized_pics\n",
      "\n",
      "Processing complete.\n",
      "Successfully processed 8 images.\n",
      "Skipped 1 files (no faces or invalid format).\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Define the paths for input and output directories\n",
    "INPUT_DIR = \"C:\\\\Exercises\\\\my pics\"\n",
    "OUTPUT_DIR = \"C:\\\\Exercises\\\\my pics\\\\cropped_resized_pics\"\n",
    "TARGET_SIZE = (128, 128)\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"\n",
    "    Creates the input and output directories if they don't exist.\n",
    "    \"\"\"\n",
    "    os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"Input directory '{INPUT_DIR}' and output directory '{OUTPUT_DIR}' are ready.\")\n",
    "\n",
    "def find_and_process_faces():\n",
    "    \"\"\"\n",
    "    Iterates through images in the input directory, detects faces,\n",
    "    crops and resizes them, and saves the results to the output directory.\n",
    "    \"\"\"\n",
    "    # Load the Haar Cascade for face detection\n",
    "    # This is a pre-trained model for frontal face detection.\n",
    "    # The file is typically included with the opencv-python package.\n",
    "    cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "\n",
    "    if face_cascade.empty():\n",
    "        print(\"Error: Could not load Haar Cascade file.\")\n",
    "        sys.exit()\n",
    "\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    # Process each file in the input directory\n",
    "    for filename in os.listdir(INPUT_DIR):\n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            print(f\"Skipping non-image file: {filename}\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(INPUT_DIR, filename)\n",
    "\n",
    "        try:\n",
    "            # Read the image using OpenCV\n",
    "            img_cv = cv2.imread(image_path)\n",
    "            if img_cv is None:\n",
    "                print(f\"Warning: Could not read image at {image_path}. Skipping.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Convert to grayscale for face detection\n",
    "            gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces in the image\n",
    "            # The detectMultiScale function finds faces and returns a list of rectangles.\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.1,\n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30)\n",
    "            )\n",
    "\n",
    "            if len(faces) == 0:\n",
    "                print(f\"No faces found in {filename}. Skipping.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            print(f\"Found {len(faces)} face(s) in {filename}. Processing...\")\n",
    "\n",
    "            # Crop and save each detected face\n",
    "            for i, (x, y, w, h) in enumerate(faces):\n",
    "                # Crop the face region\n",
    "                face_crop = img_cv[y:y+h, x:x+w]\n",
    "\n",
    "                # Convert the OpenCV image (NumPy array) to a Pillow Image\n",
    "                img_pil = Image.fromarray(cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                # Resize the cropped face to the target size\n",
    "                img_resized = img_pil.resize(TARGET_SIZE, Image.LANCZOS)\n",
    "\n",
    "                # Save the resized face to the output directory\n",
    "                output_filename = f\"{os.path.splitext(filename)[0]}_face_{i}.jpg\"\n",
    "                output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "                img_resized.save(output_path)\n",
    "                print(f\"  - Saved cropped and resized face to {output_path}\")\n",
    "            \n",
    "            processed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {filename}: {e}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Successfully processed {processed_count} images.\")\n",
    "    print(f\"Skipped {skipped_count} files (no faces or invalid format).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_directories()\n",
    "    \n",
    "    # Check if the input directory is empty\n",
    "    if not os.listdir(INPUT_DIR):\n",
    "        print(f\"\\nPlease add your images to the '{INPUT_DIR}' directory and run the script again.\")\n",
    "    else:\n",
    "        find_and_process_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e09091bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from directory...\n",
      "\n",
      "--- Process Complete ---\n",
      "Successfully loaded 8 images.\n",
      "Final array shape: (8, 128, 128, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "0.22625089\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# The directory where your pre-processed 128x128 images are saved\n",
    "IMAGES_DIR = 'C:\\\\Exercises\\\\my pics\\\\cropped_resized_pics'\n",
    "TARGET_SIZE = (128, 128)\n",
    "\n",
    "# Check if the directory exists and has images\n",
    "if not os.path.exists(IMAGES_DIR) or not os.listdir(IMAGES_DIR):\n",
    "    print(f\"Error: The directory '{IMAGES_DIR}' does not exist or is empty.\")\n",
    "    print(\"Please make sure you have run the preprocessing script first.\")\n",
    "    # Exit gracefully if the directory is not ready\n",
    "    exit()\n",
    "\n",
    "# List to hold the image data as NumPy arrays\n",
    "image_list = []\n",
    "\n",
    "print(\"Loading images from directory...\")\n",
    "for filename in os.listdir(IMAGES_DIR):\n",
    "    # Check for image file extensions\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        filepath = os.path.join(IMAGES_DIR, filename)\n",
    "        \n",
    "        # Read the image in BGR format\n",
    "        image = cv2.imread(filepath)\n",
    "\n",
    "        # Check if the image was loaded successfully\n",
    "        if image is not None:\n",
    "            # Append the image array to the list\n",
    "            image_list.append(image)\n",
    "        else:\n",
    "            print(f\"Warning: Could not read image file '{filename}'. Skipping.\")\n",
    "\n",
    "# Convert the list of image arrays into a single NumPy array\n",
    "images_array = np.array(image_list)\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")\n",
    "print(f\"Successfully loaded {len(image_list)} images.\")\n",
    "print(f\"Final array shape: {images_array.shape}\")\n",
    "emb1=embedding_model.predict(images_array)\n",
    "print(cosine_distance(emb1[5],emb1[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
